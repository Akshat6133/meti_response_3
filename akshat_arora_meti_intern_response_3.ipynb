{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw9RvDzvUndV",
        "outputId": "9f979129-fc9d-41d3-afe6-832d6620f03c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 20.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 614kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.60MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.86MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/30] Batch 0/469                   Loss D: 0.7108, loss G: 0.6997\n",
            "Epoch [0/30] Batch 400/469                   Loss D: 0.5014, loss G: 0.6275\n",
            "Epoch [1/30] Batch 0/469                   Loss D: 0.3431, loss G: 1.3717\n",
            "Epoch [1/30] Batch 400/469                   Loss D: 0.2785, loss G: 1.9023\n",
            "Epoch [2/30] Batch 0/469                   Loss D: 0.1412, loss G: 1.9447\n",
            "Epoch [2/30] Batch 400/469                   Loss D: 0.2188, loss G: 1.9723\n",
            "Epoch [3/30] Batch 0/469                   Loss D: 0.1435, loss G: 1.7376\n",
            "Epoch [3/30] Batch 400/469                   Loss D: 0.1873, loss G: 2.2029\n",
            "Epoch [4/30] Batch 0/469                   Loss D: 0.2992, loss G: 1.4917\n",
            "Epoch [4/30] Batch 400/469                   Loss D: 0.2373, loss G: 2.5755\n",
            "Epoch [5/30] Batch 0/469                   Loss D: 0.2341, loss G: 1.3738\n",
            "Epoch [5/30] Batch 400/469                   Loss D: 0.1212, loss G: 2.5346\n",
            "Epoch [6/30] Batch 0/469                   Loss D: 0.2406, loss G: 2.0199\n",
            "Epoch [6/30] Batch 400/469                   Loss D: 0.4054, loss G: 2.9623\n",
            "Epoch [7/30] Batch 0/469                   Loss D: 0.0915, loss G: 2.9598\n",
            "Epoch [7/30] Batch 400/469                   Loss D: 0.1423, loss G: 3.0068\n",
            "Epoch [8/30] Batch 0/469                   Loss D: 0.1277, loss G: 2.6883\n",
            "Epoch [8/30] Batch 400/469                   Loss D: 0.0462, loss G: 3.5559\n",
            "Epoch [9/30] Batch 0/469                   Loss D: 0.5146, loss G: 6.1012\n",
            "Epoch [9/30] Batch 400/469                   Loss D: 0.1751, loss G: 1.8037\n",
            "Epoch [10/30] Batch 0/469                   Loss D: 0.1228, loss G: 2.6666\n",
            "Epoch [10/30] Batch 400/469                   Loss D: 1.0241, loss G: 7.7598\n",
            "Epoch [11/30] Batch 0/469                   Loss D: 0.1752, loss G: 2.4153\n",
            "Epoch [11/30] Batch 400/469                   Loss D: 0.2420, loss G: 2.5461\n",
            "Epoch [12/30] Batch 0/469                   Loss D: 0.1164, loss G: 2.6443\n",
            "Epoch [12/30] Batch 400/469                   Loss D: 0.1750, loss G: 2.7872\n",
            "Epoch [13/30] Batch 0/469                   Loss D: 0.0825, loss G: 2.7262\n",
            "Epoch [13/30] Batch 400/469                   Loss D: 0.0678, loss G: 3.2976\n",
            "Epoch [14/30] Batch 0/469                   Loss D: 0.1340, loss G: 4.6658\n",
            "Epoch [14/30] Batch 400/469                   Loss D: 0.1023, loss G: 2.9419\n",
            "Epoch [15/30] Batch 0/469                   Loss D: 0.0404, loss G: 2.9976\n",
            "Epoch [15/30] Batch 400/469                   Loss D: 0.0093, loss G: 7.4271\n",
            "Epoch [16/30] Batch 0/469                   Loss D: 0.0515, loss G: 10.7961\n",
            "Epoch [16/30] Batch 400/469                   Loss D: 0.0250, loss G: 3.9718\n",
            "Epoch [17/30] Batch 0/469                   Loss D: 0.0303, loss G: 3.7494\n",
            "Epoch [17/30] Batch 400/469                   Loss D: 0.0668, loss G: 4.2250\n",
            "Epoch [18/30] Batch 0/469                   Loss D: 0.0620, loss G: 3.8989\n",
            "Epoch [18/30] Batch 400/469                   Loss D: 0.0021, loss G: 6.3144\n",
            "Epoch [19/30] Batch 0/469                   Loss D: 0.0006, loss G: 7.4276\n",
            "Epoch [19/30] Batch 400/469                   Loss D: 0.0014, loss G: 7.0533\n",
            "Epoch [20/30] Batch 0/469                   Loss D: 0.0010, loss G: 7.0640\n",
            "Epoch [20/30] Batch 400/469                   Loss D: 0.0001, loss G: 8.9531\n",
            "Epoch [21/30] Batch 0/469                   Loss D: 0.0003, loss G: 9.3795\n",
            "Epoch [21/30] Batch 400/469                   Loss D: 0.0149, loss G: 6.7877\n",
            "Epoch [22/30] Batch 0/469                   Loss D: 0.0070, loss G: 5.2380\n",
            "Epoch [22/30] Batch 400/469                   Loss D: 0.0003, loss G: 8.1519\n",
            "Epoch [23/30] Batch 0/469                   Loss D: 0.0006, loss G: 7.8829\n",
            "Epoch [23/30] Batch 400/469                   Loss D: 0.0013, loss G: 7.7840\n",
            "Epoch [24/30] Batch 0/469                   Loss D: 0.0002, loss G: 8.3642\n",
            "Epoch [24/30] Batch 400/469                   Loss D: 0.0001, loss G: 9.5166\n",
            "Epoch [25/30] Batch 0/469                   Loss D: 0.0001, loss G: 9.1425\n",
            "Epoch [25/30] Batch 400/469                   Loss D: 0.0009, loss G: 10.1072\n",
            "Epoch [26/30] Batch 0/469                   Loss D: 0.0000, loss G: 10.1644\n",
            "Epoch [26/30] Batch 400/469                   Loss D: 0.0000, loss G: 10.1915\n",
            "Epoch [27/30] Batch 0/469                   Loss D: 0.0359, loss G: 5.2194\n",
            "Epoch [27/30] Batch 400/469                   Loss D: 0.0000, loss G: 10.3626\n",
            "Epoch [28/30] Batch 0/469                   Loss D: 0.0001, loss G: 9.7812\n",
            "Epoch [28/30] Batch 400/469                   Loss D: 0.0000, loss G: 11.5572\n",
            "Epoch [29/30] Batch 0/469                   Loss D: 0.0000, loss G: 11.3939\n",
            "Epoch [29/30] Batch 400/469                   Loss D: 0.0000, loss G: 11.9136\n"
          ]
        }
      ],
      "source": [
        "# google colab link: https://colab.research.google.com/drive/1SG9QdlxYQ3CIsR9rPTN4aRj_xRSxHVZt?usp=sharing\n",
        "# streamlit website link: https://metiresponse3-vn6vhxwhfbpoqmj8vf8fzz.streamlit.app/\n",
        "import torch # type: ignore\n",
        "import torch.nn as nn # type: ignore\n",
        "import torch.optim as optim # type: ignore\n",
        "from torchvision import datasets, transforms # type: ignore\n",
        "from torch.utils.data import DataLoader # type: ignore\n",
        "import torch.nn.functional as F # type: ignore\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparams\n",
        "batch_size = 128\n",
        "latent_dim = 100\n",
        "num_epochs = 30\n",
        "learning_rate = 0.0002\n",
        "\n",
        "# MNIST dataset loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1,1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(10, 10)  # For conditioning on digit label\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + 10, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 28*28),\n",
        "            nn.Tanh()  # Output range [-1,1]\n",
        "        )\n",
        "    def forward(self, noise, labels):\n",
        "        c = self.label_emb(labels)\n",
        "        x = torch.cat([noise, c], 1)\n",
        "        img = self.model(x)\n",
        "        img = img.view(-1, 1, 28, 28)\n",
        "        return img\n",
        "\n",
        "# Discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(10, 10)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28 + 10, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, img, labels):\n",
        "        c = self.label_emb(labels)\n",
        "        x = img.view(img.size(0), -1)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        validity = self.model(x)\n",
        "        return validity\n",
        "\n",
        "# Initialize models and optimizers\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
        "\n",
        "        batch_size_curr = imgs.size(0)\n",
        "        real_imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Real and fake labels\n",
        "        valid = torch.ones(batch_size_curr, 1, device=device)\n",
        "        fake = torch.zeros(batch_size_curr, 1, device=device)\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        z = torch.randn(batch_size_curr, latent_dim, device=device)\n",
        "        gen_labels = torch.randint(0, 10, (batch_size_curr,), device=device)\n",
        "\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs, gen_labels), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), gen_labels), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        if batch_idx % 400 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(train_loader)} \\\n",
        "                  Loss D: {d_loss.item():.4f}, loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "# Save Generator model weights\n",
        "torch.save(generator.state_dict(), \"generator.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
